{"cells":[{"cell_type":"markdown","id":"df82f8ba","metadata":{"id":"df82f8ba"},"source":["## Overview\n","\n","\n","0.0 → Introduction to AI and its Evolution\n","\n","0.1 → Machine Learning vs Deep Learning\n","\n","0.2 → Tokens VS Parameters in Models\n","\n","0.3 → What can AI realistically achieve today?\n","\n","0.4 → 25 Papers That Completely Transformed the Computer World"]},{"cell_type":"markdown","id":"a3f23879","metadata":{"id":"a3f23879"},"source":["### 0.0 → Introduction to AI and its Evolution\n"]},{"cell_type":"code","execution_count":null,"id":"4cc055d4","metadata":{"id":"4cc055d4"},"outputs":[],"source":["replicate human intelligence\n","\n","milestones\n","\n","1943 - A logical calculus of the ideas immanent in neurons activity - foundation of neural network\n","\n","1950 - turing test\n","\n","1956 - Artificial Intelligence - John McCarthy\n","\n","\n","1965 - NLP\n","\n","1997 - IBM Deep blue defeats world champion\n","\n","2004 - self-driving cars\n","\n","2006 - deep learning - Geoffery Hinton\n","\n","2007 - Apple - voice recognition (siri added in 2011)\n","\n","2012 - AlexNet - deep CNN - computer vision\n","\n","2014 - Google Deep mind - AlphaGo defeats a human through reinforcement learning\n","\n","2017 - AlphaGo zero - just by self-play it defeats its predecessor without any human data.\n","\n","2020 - GPT-3\n","\n","2021 - Google - MUM (Multitask Unified Model)\n","\n","2022 - DALL-E 2 and stable diffusion bring GenerativeAI\n","\n","2023 - DeepMind - AlphaFold - accurately predicts the structure of all know proteins.\n","\n","2024 - LLMs"]},{"cell_type":"code","execution_count":null,"id":"24fdf3f7","metadata":{"id":"24fdf3f7"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2fe18ad6","metadata":{"id":"2fe18ad6"},"source":["### 0.1 → Machine Learning vs Deep Learning"]},{"cell_type":"markdown","id":"15daa185","metadata":{"id":"15daa185"},"source":["How to know features without having domain knowledge?\n","\n","https://github.com/VIKASAI/Data-Science-Labs"]},{"cell_type":"code","execution_count":null,"id":"828a9e73","metadata":{"id":"828a9e73"},"outputs":[],"source":["Machine Learning\n","\n","- feature engineering\n","- structured data - tabular, relational DB\n","- domain knowledge for design features\n","- can be trained on small data\n","- CPUs\n","- understanding of decision\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a425ebaa","metadata":{"id":"a425ebaa"},"outputs":[],"source":["Deep Learning\n","\n","- no need for feature engineering\n","- unstructured data - images, audio, video, text\n","- minimizes the need for manual feature engineering\n","- need large amounts of data to perform well\n","- GPUs and TPUs\n","- it consider black box and harder to explain how it come up with the prediction\n"]},{"cell_type":"markdown","id":"1bd3eb27","metadata":{"id":"1bd3eb27"},"source":["### 0.2 → Tokens VS Parameters in Models\n"]},{"cell_type":"code","execution_count":null,"id":"675f6903","metadata":{"id":"675f6903"},"outputs":[],"source":["this is an _ apple\n","\n","4 words\n","- 4 word token\n","\n","16 characters\n","- 16 character token\n","\n","sub token\n","\n","\n","\n","GPT3\n","- 500 billion word token\n","- 175 Billion parameters"]},{"cell_type":"markdown","id":"2a35a87e","metadata":{"id":"2a35a87e"},"source":["How ChatGPT works\n"]},{"cell_type":"markdown","id":"3297642e","metadata":{"id":"3297642e"},"source":["### 0.3 → What can AI realistically achieve today?"]},{"cell_type":"code","execution_count":null,"id":"be82c347","metadata":{"id":"be82c347"},"outputs":[],"source":["NLP - chatbots and Virtual assistants\n","- google assistant\n","- siri\n","- alexa\n","\n","- text generation\n","- sentiment analysis\n","- language translation\n","\n","Computer Vision\n","- autonomous vehicles\n","- tesla, waymo\n","what a tesla car sees: https://www.youtube.com/watch?v=fKXztwtXaGo\n","\n","- Object detection and recognition\n","- image enhancement"]},{"cell_type":"code","execution_count":null,"id":"da6b4602","metadata":{"id":"da6b4602"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"8ed63169","metadata":{"id":"8ed63169"},"source":["### 0.4 → 25 Papers That Completely Transformed the Computer World (Transformer only)\n","\n","Attention Is All You Need: Into a new deep learning architecture known as the transformer\n","\n","https://arxiv.org/pdf/1706.03762"]},{"cell_type":"code","execution_count":null,"id":"a605e37b","metadata":{"id":"a605e37b"},"outputs":[],"source":["deep learning architecture - Transformer"]},{"cell_type":"code","execution_count":null,"id":"36edff4f","metadata":{"id":"36edff4f"},"outputs":[],"source":["tranditional models\n","- RNN\n","- CNN\n","- LSTM, GRU"]},{"cell_type":"code","execution_count":null,"id":"c1a856b4","metadata":{"id":"c1a856b4"},"outputs":[],"source":["- slow\n","- hard to train"]},{"cell_type":"code","execution_count":null,"id":"e18f54a5","metadata":{"id":"e18f54a5"},"outputs":[],"source":["attention machenism\n","\n","Himanshu is sitting on a chair. He is going to share resources."]},{"cell_type":"code","execution_count":null,"id":"7958d979","metadata":{"id":"7958d979"},"outputs":[],"source":["why transformers are important?\n","\n","- parallel processing\n","- better performance\n","- simple design"]},{"cell_type":"code","execution_count":null,"id":"4c44536b","metadata":{"id":"4c44536b"},"outputs":[],"source":["BERT by google"]},{"cell_type":"markdown","id":"1980c6a4","metadata":{"id":"1980c6a4"},"source":["### PyTorch Basics\n","\n","https://github.com/VIKASAI/Deep-Learning-Labs"]},{"cell_type":"code","execution_count":null,"id":"68a73cad","metadata":{"id":"68a73cad"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}